# -*- coding: utf-8 -*-
"""Fixmatch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vavnf3FvKLmXHwnWZcw25xz5IOPFuOcK
"""

import torch
import torch.nn.functional as F
from sklearn.metrics import precision_score, recall_score, f1_score
import argparse

from darp_function import estimate_pseudo, opt_solver
from args import parse_args
args = parse_args()
from utils import AverageMeter,Bar



class SemiLoss(object):
    def __call__(self, args, outputs_x, targets_x, outputs_u, targets_u, epoch, mask=None):
        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))
        Lu = -torch.mean(torch.sum(F.log_softmax(outputs_u, dim=1) * targets_u, dim=1) * mask)

        return Lx,Lu


def trains(args, labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion, epoch, use_cuda,
          target_disb, emp_distb_u, pseudo_orig, pseudo_refine):
    return train_fix(args, labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion, epoch,
                      use_cuda, target_disb, emp_distb_u, pseudo_orig, pseudo_refine)


def train_fix(args, labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion, epoch,
              use_cuda, target_disb, emp_distb_u, pseudo_orig, pseudo_refine):
  batch_time=AverageMeter()
  data_time=AverageMeter()
  losses=AverageMeter()
  losses_x=AverageMeter()
  losses_u=AverageMeter()
  end=time.time()

  batch_iteration = len(unlabeled_trainloader)

  bar=Bar('Training',max=batch_iteration)
  labeled_train_iter=iter(labeled_trainloader)
  unlabeled_train_iter=iter(unlabeled_trainloader)



  model.train()

  for batch_idx in range(batch_iteration):
    try:
      inputs_x,targets_x,_=labeled_train_iter.next()
    except:
      labeled_train_iter=iter(labeled_trainloader)
      inputs_x,targets_x,_=labeled_train_iter.next()
    try:
      (inputs_u,inputs_u2,inputs_u3),_,idx_u=unlabeled_train_iter.next()

    except:
      unlabeled_train_iter=iter(unlabeled_trainloader)
      (inputs_u,inputs_u2,inputs_u3),_,idx_u=unlabeled_train_iter.next()

    data_time.update(time.time()-end)
    batch_size=inputs_x.size(0)

    targets_x=torch.zeros(batch_size,args.num_class).scatter_(1,targets_x.view(-1,1),1)
    if use_cuda:
      inputs_x,targets_x=inputs_x.cuda(),targets_x.cuda(non_blocking=True)
      inputs_u,inputs_u2,inputs_u3=inputs_u.cuda(),inputs_u2.cuda(),inputs_u3.cuda()

    with torch.no_grad():
      outputs_u,_=model(inputs_u)
      targets_u=torch.softmax(outputs_u,dim=1)

      pseudo_orig[idx_u,:]=targets_u.data.cpu()
      pseudo_orig_backup=pseudo_orig.clone()

      # Applying DARP

      if args.darp and epoch > args.warm:
        if batch_idx % args.num_iter == 0:
          # Iterative normalization
          targets_u, weights_u = estimate_pseudo(target_disb, pseudo_orig, args.num_class, args.alpha)
          scale_term = targets_u * weights_u.reshape(1, -1)
          pseudo_orig = (pseudo_orig * scale_term + 1e-6) \
                          / (pseudo_orig * scale_term + 1e-6).sum(dim=1, keepdim=True)

          if args.dataset == 'stl10' or args.dataset == 'cifar100':
            opt_res = opt_solver(pseudo_orig, target_disb, args.iter_T, 0.3)
          else:
            opt_res = opt_solver(pseudo_orig, target_disb, args.iter_T, 0.1)

          # Updated pseudo-labels are saved
          pseudo_refine = opt_res

          # Select
          targets_u = opt_res[idx_u].detach().cuda()
          pseudo_orig = pseudo_orig_backup
        else:
              # Using previously saved pseudo-labels
              targets_u = pseudo_refine[idx_u].cuda()

    max_p,p_hat=torch.max(targets_u,dim=1)
    p_hat=torch.zeros(batch_size,args.num_class).cuda().scatter_(1,p_hat.view(-1,1),1)

    select_mask=max_p.ge(args.tau)
    select_mask=torch.cat([select_mask,select_mask],0).float()


    all_inputs=torch.cat([inputs_x,inputs_u2,inputs_u3],dim=0)
    all_targets=torch.cat([targets_x,p_hat,p_hat],dim=0)

    all_outputs,_=model(all_inputs)
    logits_x=all_outputs[:batch_size]
    logits_u=all_outputs[batch_size:]

    Lx,Lu=criterion(args,logits_x,all_targets[:batch_size],logits_u,all_targets[batch_size:],epoch,select_mask)
    loss=Lx+Lu

    # record loss
    losses.update(loss.item(), inputs_x.size(0))
    losses_x.update(Lx.item(), inputs_x.size(0))
    losses_u.update(Lu.item(), inputs_x.size(0))

    # compute gradient and do SGD step
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    ema_optimizer.step()

    # measure elapsed time
    batch_time.update(time.time() - end)
    end = time.time()

    # plot progress
    bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | ' \
                  'Loss: {loss:.4f} | Loss_x: {loss_x:.4f} | Loss_u: {loss_u:.4f}'.format(
                batch=batch_idx + 1,
                size=batch_iteration,
                data=data_time.avg,
                bt=batch_time.avg,
                total=bar.elapsed_td,
                eta=bar.eta_td,
                loss=losses.avg,
                loss_x=losses_x.avg,
                loss_u=losses_u.avg,
                )
    bar.next()
  bar.finish()

  return (losses.avg, losses_x.avg, losses_u.avg, emp_distb_u, pseudo_orig, pseudo_refine)

def make_imb_data(max_num, class_num, gamma):
    mu = np.power(1/gamma, 1/(class_num - 1))
    class_num_list = []
    for i in range(class_num):
        if i == (class_num - 1):
            class_num_list.append(int(max_num / gamma))
        else:
            class_num_list.append(int(max_num * np.power(mu, i)))
    return list(class_num_list)



import argparse
import os
import shutil
import time
import random
import math

import matplotlib.pyplot as plt
import itertools

import numpy as np

import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data as data
import torchvision.transforms as transforms
import torch.nn.functional as F

import models.wrn as models

from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig
from tensorboardX import SummaryWriter
from scipy import optimize

def validate(valloader, model, criterion, use_cuda, mode, num_class=args.num_class):
  C_predict=np.ones(1)
  C_target=np.ones(1)
  batch_time = AverageMeter()
  data_time = AverageMeter()
  losses = AverageMeter()
  top1 = AverageMeter()
  top5 = AverageMeter()

  # switch to evaluate mode
  model.eval()

  end = time.time()
  bar = Bar(f'{mode}', max=len(valloader))

  classwise_correct = torch.zeros(num_class)
  classwise_num = torch.zeros(num_class)
  section_acc = torch.zeros(3)

  with torch.no_grad():
    for batch_idx, (inputs, targets, _) in enumerate(valloader):
      # measure data loading time
      data_time.update(time.time() - end)

      if use_cuda:
        inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)
      # compute output
      outputs, _ = model(inputs)

      loss = criterion(outputs, targets)

      # measure accuracy and record loss
      prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))
      losses.update(loss.item(), inputs.size(0))
      top1.update(prec1.item(), inputs.size(0))
      top5.update(prec5.item(), inputs.size(0))

      # classwise prediction
      pred_label = outputs.max(1)[1]
      pred_mask = (targets == pred_label).float()

      C_predict = np.hstack((C_predict, pred_label.to('cpu').numpy()))
      C_target = np.hstack((C_target, targets.to('cpu').numpy()))

      for i in range(num_class):
        class_mask = (targets == i).float()
        classwise_correct[i] += (class_mask.to('cpu') * pred_mask.to('cpu')).sum()
        classwise_num[i] += class_mask.to('cpu').sum()

      # measure elapsed time
      batch_time.update(time.time() - end)
      end = time.time()

      # plot progress
      bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | ' \
                    'Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(
                  batch=batch_idx + 1,
                  size=len(valloader),
                  data=data_time.avg,
                  bt=batch_time.avg,
                  total=bar.elapsed_td,
                  eta=bar.eta_td,
                  loss=losses.avg,
                  top1=top1.avg,
                  top5=top5.avg,
                  )
      bar.next()
    bar.finish()

  # Major, Neutral, Minor
  section_num = int(num_class / 3)
  classwise_acc = (classwise_correct / classwise_num)
  section_acc[0] = classwise_acc[:section_num].mean()
  section_acc[2] = classwise_acc[-1 * section_num:].mean()
  section_acc[1] = classwise_acc[section_num:-1 * section_num].mean()
  GM = 1
  for i in range(num_class):
    if classwise_acc[i] == 0:
      # To prevent the N/A values, we set the minimum value as 0.001
      GM *= (1/(100 * num_class)) ** (1/num_class)
    else:
      GM *= (classwise_acc[i]) ** (1/num_class)

  return (losses.avg, top1.avg, section_acc.numpy(), GM,C_predict[1:],C_target[1:])



class WeightEMA(object):
    def __init__(self, model, ema_model, lr=0.002, alpha=0.999):
        self.model = model
        self.ema_model = ema_model
        self.alpha = alpha
        self.params = list(model.state_dict().values())
        self.ema_params = list(ema_model.state_dict().values())
        self.wd = 0.02 * lr
        for param, ema_param in zip(self.params, self.ema_params):
            param.data.copy_(ema_param.data)
    def step(self):
        one_minus_alpha = 1.0 - self.alpha
        for param, ema_param in zip(self.params, self.ema_params):
            # print(ema_param.mean())
            if ema_param.shape!=torch.Size([]):
                ema_param.mul_(self.alpha)
                ema_param.add_(param * one_minus_alpha)
                # customized weight decay
                param.mul_(1 - self.wd)
def save_checkpoint(state, epoch, checkpoint='none', filename='checkpoint.pth.tar'):
    filepath = os.path.join(checkpoint, filename)
    torch.save(state, filepath)

    if epoch % 100 == 0:
        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_' + str(epoch) + '.pth.tar'))


def plot_confusion_matrix(con_mat, labels, title='Confusion Matrix', cmap=plt.cm.get_cmap('Blues'), normalize=True):
    f, ax = plt.subplots(1, 2, figsize=(20, 8))
    ax[0].imshow(con_mat, interpolation='nearest', cmap=cmap)
    ax[1].imshow(con_mat, interpolation='nearest', cmap=cmap)
    ax[0].set_title(title)
    ax[1].set_title(title)

    marks = np.arange(len(labels))
    nlabels = []
    n_class = []
    for k in range(len(con_mat)):
        n = sum(con_mat[k])
        nlabel = '{0}(n={1})'.format(labels[k], n)
        n_class.append(n)
        nlabels.append(nlabel)

    ax[0].set_xticks(marks, labels)
    ax[0].set_yticks(marks, nlabels)
    ax[1].set_xticks(marks, labels)
    ax[1].set_yticks(marks, nlabels)
    thresh = con_mat.max() / 2.

    for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):
        ax[0].text(j, i, '{:.1f}%'.format(con_mat[i, j] * 100 / n_class[i]), horizontalalignment="center",
                   color="white" if con_mat[i, j] > thresh else "black")

    for i, j in itertools.product(range(con_mat.shape[0]), range(con_mat.shape[1])):
        ax[1].text(j, i, con_mat[i, j], horizontalalignment="center",
                   color="white" if con_mat[i, j] > thresh else "black")

    ax[0].set_ylabel('True label')
    ax[1].set_ylabel('True label')
    ax[0].set_xlabel('Predicted label')
    ax[1].set_xlabel('Predicted label')
    plt.show()

